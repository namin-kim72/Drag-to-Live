import cv2
import numpy as np
import torch
from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler
from PIL import Image
import os

# ==========================================
# 1. ì„¤ì •
# ==========================================
IMAGE_PATH = "test_input.png"  # í”¼ë¼ë¯¸ë“œ ì‚¬ì§„
OUTPUT_VIDEO = "demo_result.mp4"
OUTPUT_INPUT_IMG = "demo_input_with_arrow.jpg"  # PPTì— ë„£ì„ 'ì…ë ¥' ì´ë¯¸ì§€

# ëª¨ë¸ ì„¤ì •
BASE_MODEL = "runwayml/stable-diffusion-v1-5"
MOTION_ADAPTER = "guoyww/animatediff-motion-adapter-v1-5-2"
LORA_PATH = "output_drag_lora"

# ==========================================
# 2. ë“œë˜ê·¸ ì¸í„°í˜ì´ìŠ¤ (ë§ˆìš°ìŠ¤ë¡œ ê·¸ë¦¬ê¸°)
# ==========================================
drawing = False
ix, iy = -1, -1
img_display = None


def draw_arrow(event, x, y, flags, param):
    global ix, iy, drawing, img_display

    if event == cv2.EVENT_LBUTTONDOWN:
        drawing = True
        ix, iy = x, y

    elif event == cv2.EVENT_MOUSEMOVE:
        if drawing:
            temp_img = img_display.copy()
            # ë¹¨ê°„ìƒ‰ í™”ì‚´í‘œ ê·¸ë¦¬ê¸° (ê¶¤ì  ì‹œê°í™”)
            cv2.arrowedLine(temp_img, (ix, iy), (x, y), (0, 0, 255), 2, tipLength=0.3)
            cv2.imshow('Drag Your Cloud', temp_img)

    elif event == cv2.EVENT_LBUTTONUP:
        drawing = False
        # ìµœì¢… í™”ì‚´í‘œ ê³ ì •
        cv2.arrowedLine(img_display, (ix, iy), (x, y), (0, 0, 255), 2, tipLength=0.3)
        cv2.imshow('Drag Your Cloud', img_display)
        print(f"ğŸ‘‰ ê¶¤ì  ì…ë ¥ë¨: ({ix},{iy}) -> ({x},{y})")


# ==========================================
# 3. AI ì˜ìƒ ìƒì„±ê¸°
# ==========================================
def generate_video(pipe, prompt):
    print("â³ AIê°€ êµ¬ë¦„ì„ ìƒì„±í•˜ëŠ” ì¤‘... (ì•½ 1ë¶„)")
    generator = torch.Generator("cuda").manual_seed(42)
    output = pipe(
        prompt=prompt,
        negative_prompt="bad quality, low resolution",
        num_frames=16,
        guidance_scale=7.5,
        num_inference_steps=25,
        generator=generator,
        width=256, height=256
    )
    frames = output.frames[0]

    # ì˜ìƒ ì €ì¥ (ìƒ‰ìƒ ë³´ì • í¬í•¨)
    height, width, _ = np.array(frames[0]).shape
    out = cv2.VideoWriter(OUTPUT_VIDEO, cv2.VideoWriter_fourcc(*'mp4v'), 8, (width, height))
    for frame in frames:
        img_bgr = cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR)
        out.write(img_bgr)
    out.release()
    print(f"âœ… ì˜ìƒ ìƒì„± ì™„ë£Œ: {OUTPUT_VIDEO}")


# ==========================================
# 4. ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    global img_display

    # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° ë¦¬ì‚¬ì´ì¦ˆ
    if not os.path.exists(IMAGE_PATH):
        print("âŒ test_input.jpgê°€ ì—†ìŠµë‹ˆë‹¤! ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
        return

    original_img = cv2.imread(IMAGE_PATH)
    img_display = cv2.resize(original_img, (256, 256))

    # 2. ë§ˆìš°ìŠ¤ ì…ë ¥ ë°›ê¸°
    cv2.namedWindow('Drag Your Cloud')
    cv2.setMouseCallback('Drag Your Cloud', draw_arrow)

    print("ğŸ¨ ì´ë¯¸ì§€ ìœ„ì— ë§ˆìš°ìŠ¤ë¡œ ë“œë˜ê·¸í•˜ì—¬ í™”ì‚´í‘œë¥¼ ê·¸ë¦¬ì„¸ìš”.")
    print("   (ë‹¤ ê·¸ë ¸ìœ¼ë©´ 'Enter'ë¥¼ ëˆ„ë¥´ì„¸ìš”. 'q'ëŠ” ì¢…ë£Œ)")

    while True:
        cv2.imshow('Drag Your Cloud', img_display)
        key = cv2.waitKey(1) & 0xFF
        if key == 13:  # Enter í‚¤
            break
        elif key == ord('q'):
            return

    # 3. ì…ë ¥ ì´ë¯¸ì§€ ì €ì¥ (PPTìš©)
    cv2.imwrite(OUTPUT_INPUT_IMG, img_display)
    cv2.destroyAllWindows()
    print(f"ğŸ“¸ ì…ë ¥ ê¶¤ì  ì´ë¯¸ì§€ ì €ì¥ë¨: {OUTPUT_INPUT_IMG}")

    # 4. ëª¨ë¸ ë¡œë“œ ë° ìƒì„±
    device = "cuda" if torch.cuda.is_available() else "cpu"
    adapter = MotionAdapter.from_pretrained(MOTION_ADAPTER)
    pipe = AnimateDiffPipeline.from_pretrained(BASE_MODEL, motion_adapter=adapter, torch_dtype=torch.float16).to(device)
    pipe.scheduler = DDIMScheduler.from_pretrained(BASE_MODEL, subfolder="scheduler", clip_sample=False,
                                                   timestep_spacing="linspace", steps_offset=1)

    try:
        pipe.unet.load_attn_procs(LORA_PATH)
        print("âœ… LoRA ì ìš© ì™„ë£Œ")
    except:
        print("âš ï¸ LoRA íŒŒì¼ì´ ì—†ì–´ì„œ ê¸°ë³¸ ëª¨ë¸ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")

    # ìƒì„± ì‹œì‘
    generate_video(pipe, "timelapse clouds moving over egyptian pyramids, desert, cinematic, high quality, 4k")


if __name__ == "__main__":
    main()