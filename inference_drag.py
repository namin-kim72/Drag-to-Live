import torch
from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler
from diffusers.utils import export_to_gif
from PIL import Image
import cv2
import numpy as np
import os

# ==========================================
# 1. ì„¤ì • (í•™ìŠµ ë•Œì™€ ë§ì¶°ì•¼ ì˜ ë‚˜ì˜µë‹ˆë‹¤)
# ==========================================
BASE_MODEL = "runwayml/stable-diffusion-v1-5"
MOTION_ADAPTER = "guoyww/animatediff-motion-adapter-v1-5-2"
LORA_PATH = "output_drag_lora"  # ë°©ê¸ˆ í•™ìŠµ ëë‚œ í´ë”
TEST_IMAGE_PATH = "test_input.png"  # ì¤€ë¹„í•œ ì´ë¯¸ì§€

# í•™ìŠµ ë•Œ ì¼ë˜ í”„ë¡¬í”„íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì¤‘ìš”!)
PROMPT = "timelapse clouds moving in the sky, cinematic, high quality, 4k"
NEGATIVE_PROMPT = "bad quality, worst quality, blurry, low resolution, distortion, watermark"


# ==========================================
# 2. ìƒ‰ìƒ ë³´ì • ì €ì¥ í•¨ìˆ˜ (íŒŒë€ìƒ‰ ë°©ì§€)
# ==========================================
def save_video_fixed(frames, path, fps=8):
    height, width, _ = np.array(frames[0]).shape
    # OpenCV ë¹„ë””ì˜¤ ì‘ì„±ê¸°
    out = cv2.VideoWriter(path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

    for frame in frames:
        # PIL -> Numpy ë³€í™˜
        img_np = np.array(frame)
        # RGB -> BGR ë³€í™˜ (ì´ê²Œ í•µì‹¬!)
        img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
        out.write(img_bgr)

    out.release()
    print(f"âœ¨ ì˜ìƒ ì €ì¥ ì™„ë£Œ: {path}")


# ==========================================
# 3. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==========================================
def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # 1. ëª¨ë¸ ë¡œë“œ
    print("Loading Base Model...")
    adapter = MotionAdapter.from_pretrained(MOTION_ADAPTER)
    pipe = AnimateDiffPipeline.from_pretrained(
        BASE_MODEL,
        motion_adapter=adapter,
        torch_dtype=torch.float16
    ).to(device)

    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    pipe.scheduler = DDIMScheduler.from_pretrained(
        BASE_MODEL,
        subfolder="scheduler",
        clip_sample=False,
        timestep_spacing="linspace",
        beta_schedule="linear",
        steps_offset=1
    )

    # 2. í•™ìŠµí•œ LoRA ë¶ˆëŸ¬ì˜¤ê¸° (ì„±ì í‘œ í™•ì¸)
    print(f"Loading LoRA from {LORA_PATH}...")
    try:
        pipe.unet.load_attn_procs(LORA_PATH)
        print("âœ… LoRA Load Success!")
    except Exception as e:
        print(f"âŒ LoRA Load Failed: {e}")
        return

    # 3. ì´ë¯¸ì§€ ì¤€ë¹„
    if not os.path.exists(TEST_IMAGE_PATH):
        print(f"âš ï¸ {TEST_IMAGE_PATH}ê°€ ì—†ìŠµë‹ˆë‹¤! ê²€ì€ í™”ë©´ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        input_image = Image.new('RGB', (256, 256), color='black')
    else:
        input_image = Image.open(TEST_IMAGE_PATH).convert("RGB")
        input_image = input_image.resize((256, 256))  # í•™ìŠµ í•´ìƒë„ ë§ì¶¤

    # 4. ì˜ìƒ ìƒì„± (Inference)
    print("Generating Video... (ì•½ 1ë¶„ ì†Œìš”)")

    # ì‹œë“œ ê³ ì • (ë§¤ë²ˆ ë˜‘ê°™ì´ ì˜ ë‚˜ì˜¤ê²Œ í•˜ê¸° ìœ„í•´)
    generator = torch.Generator(device=device).manual_seed(42)

    output = pipe(
        prompt=PROMPT,
        negative_prompt=NEGATIVE_PROMPT,
        num_frames=16,  # 2ì´ˆ ì˜ìƒ
        guidance_scale=7.5,
        num_inference_steps=25,  # 25ë²ˆë§Œ ê·¸ë ¤ë„ ì¶©ë¶„
        generator=generator,
        width=256,
        height=256
    )

    frames = output.frames[0]

    # 5. ì €ì¥ (GIF + MP4)
    export_to_gif(frames, "final_result.gif")
    save_video_fixed(frames, "final_result.mp4", fps=8)

    print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ! 'final_result.mp4'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()