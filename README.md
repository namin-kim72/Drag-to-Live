# Drag-to-Live: Controllable Cloud Animation on Edge Device

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)](https://pytorch.org/)
[![Diffusers](https://img.shields.io/badge/Diffusers-0.31.0-yellow)](https://huggingface.co/docs/diffusers/index)
[![Gradio](https://img.shields.io/badge/Demo-OpenCV-green)]()

## ğŸ“– Introduction
ìµœì‹  ì˜ìƒ ìƒì„± AI(Sora, Runway ë“±)ëŠ” í€„ë¦¬í‹°ê°€ ë†’ì§€ë§Œ, ì‚¬ìš©ìê°€ ì›í•˜ëŠ” êµ¬ì²´ì ì¸ ì›€ì§ì„ì„ ì œì–´(Control)í•˜ê¸° ì–´ë µë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ê¸°ì¡´ ì—°êµ¬ì¸ *Wan-Move* ë“±ì€ H100ê¸‰ì˜ ê³ ì„±ëŠ¥ GPUë¥¼ ìš”êµ¬í•©ë‹ˆë‹¤.

**Drag-to-Live**ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê³ ì•ˆëœ **ê²½ëŸ‰í™” í’ê²½ ì œì–´ ëª¨ë¸**ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì…ë ¥í•œ ê¶¤ì (Trajectory)ì„ ê¸°ë°˜ìœ¼ë¡œ ì •ì§€ëœ ì´ë¯¸ì§€ì— ìì—°ìŠ¤ëŸ¬ìš´ ì›€ì§ì„ì„ ë¶€ì—¬í•˜ë©°, ìµœì í™”ë¥¼ í†µí•´ **RTX 4060(8GB VRAM) í™˜ê²½ì—ì„œë„ í•™ìŠµ ë° êµ¬ë™**ì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

## âœ¨ Key Features
* **ğŸ‘† Intuitive Interface:** ì´ë¯¸ì§€ ìœ„ì— ë§ˆìš°ìŠ¤ë¡œ ë“œë˜ê·¸í•˜ì—¬ ì›€ì§ì„ì˜ ë°©í–¥ê³¼ í¬ê¸°ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
* **âš¡ Edge Optimization:** RTX 4060 8GB VRAM í™˜ê²½ì—ì„œë„ í•™ìŠµ ë° ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë„ë¡ ê²½ëŸ‰í™”ë˜ì—ˆìŠµë‹ˆë‹¤ (Gradient Checkpointing, fp16, 8-frame optimization).
* **ğŸ§  Data-Driven Guidance:** CoTrackerë¡œ ì¶”ì¶œí•œ ë¬¼ë¦¬ì  ê¶¤ì ì„ LoRA(Low-Rank Adaptation)ì— í•™ìŠµì‹œì¼œ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ë¦„ì˜ íë¦„ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
* **ğŸ¥ High Quality:** Stable Diffusion v1.5ì™€ AnimateDiffë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œë„¤ë§ˆí‹±í•œ íƒ€ì„ë©ìŠ¤ ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤.

## ğŸ—ï¸ System Architecture
<img width="4729" height="4465" alt="User Trajectory Cloud-2025-12-18-061955" src="https://github.com/user-attachments/assets/0fc1e0ea-faef-4c38-8c69-5bb860453889" />

ë³¸ í”„ë¡œì íŠ¸ëŠ” CoTracker(Trajectory Encoder)ì™€ **AnimateDiff(Motion Generator)**, ê·¸ë¦¬ê³  LoRA(Style Controller)ì˜ ìœ ê¸°ì ì¸ ê²°í•©ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
1.  **Input:** ì •ì§€ ì´ë¯¸ì§€ + ì‚¬ìš©ì ì…ë ¥ ê¶¤ì 
2.  **Encoding:** CoTrackerê°€ ê¶¤ì ì„ ì¢Œí‘œ ë°ì´í„°ë¡œ ë³€í™˜
3.  **Generation:** AnimateDiffê°€ ì‹œê°„ ì¶•ì„ ìƒì„±í•˜ê³ , í•™ìŠµëœ LoRAê°€ êµ¬ë¦„ì˜ ë¬¼ë¦¬ì  ì›€ì§ì„ì„ ì£¼ì…
4.  **Output:** 8~16 í”„ë ˆì„ì˜ ê³ í™”ì§ˆ íƒ€ì„ë©ìŠ¤ ì˜ìƒ

## ğŸ“¦ Installation

### Prerequisites
* Windows 10/11 or Linux
* NVIDIA GPU (VRAM 8GB ì´ìƒ ê¶Œì¥)
* Python 3.10+
* Anaconda (Recommended)

### Setup
```bash
# 1. Clone the repository
git clone [https://github.com/namin-kim72/Drag-to-Live.git](https://github.com/namin-kim72/Drag-to-Live.git)
cd Drag-to-Live

# 2. Create Conda environment
conda create -n drag2live python=3.10
conda activate drag2live

# 3. Install dependencies
# (CoTracker ì„¤ì¹˜ë¥¼ ìœ„í•´ gitì´ í•„ìš”í•©ë‹ˆë‹¤)
pip install -r requirements.txt
```

## ğŸš€ Usage

### 1. Data Preparation (Optional)
ì§ì ‘ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ë ¤ë©´ `raw_cloud_video` í´ë”ì— MP4 íŒŒì¼ë“¤ì„ ë„£ê³  ì‹¤í–‰í•˜ì„¸ìš”. CoTrackerê°€ ê¶¤ì ì„ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
```bash
python build_dataset_drag2live.py
```

### 2. Training LoRA
êµ¬ì¶•ëœ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ LoRA ì–´ëŒ‘í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. (RTX 4060 ê¸°ì¤€ ì•½ 30ë¶„~1ì‹œê°„ ì†Œìš”)
```Bash
python train_drag_lora.py
```

### 3. Interactive Demo (GUI)
í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ í™”ì‚´í‘œë¥¼ ê·¸ë¦¬ê³  ì˜ìƒì„ ìƒì„±í•´ë´…ë‹ˆë‹¤.
```Bash
python interactive_demo.py
```
  ì‚¬ìš©ë²•: ì°½ì´ ëœ¨ë©´ ë§ˆìš°ìŠ¤ë¡œ ì›í•˜ëŠ” êµ¬ë¦„ì˜ ì´ë™ ê¶¤ì ì„ ê·¸ë¦¬ì„¸ìš”. ì…ë ¥ì„ ë§ˆì¹˜ë©´ Enterë¥¼ ëˆŒëŸ¬ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.

### ğŸ“Š Results



https://github.com/user-attachments/assets/bebe15c9-77e8-4290-8e98-76d7513cf585



### ğŸ› ï¸ Tech Stack

* **Core Model:** Stable Diffusion v1.5

* **Motion Module:** AnimateDiff

* **Trajectory Tracking:** CoTracker

* **Accelerator:** HuggingFace Accelerate & PEFT

* **Environment:** Python 3.10+, PyTorch 2.0+

### ğŸ‘¨â€ğŸ’» Author
* **Kim Nam-in** - Project Lead & Implementation - [IoT Track / Visual Intelligence Learning]

### ğŸ“œ References
1. Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance (arXiv, 2025) 

2. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning (ICLR, 2024) 

3. CoTracker: It is Better to Track Together (CVPR, 2024)
